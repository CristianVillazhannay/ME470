{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "The fully discretized finite difference equation is:\n",
    "\n",
    "$$\n",
    "\\frac{T^{n+1} - T^{n}}{\\Delta t} = D \\left[ \n",
    "    \\frac{ u_{i + 1,j    ,z    } - 2 u_{i  ,j  ,z  } + u_{i - 1,j    ,z    }}{\\Delta X^{2}} + \n",
    "    \\frac{ u_{i    ,j + 1,z    } - 2 u_{i  ,j  ,z  } + u_{i    ,j - 1,z    }}{\\Delta Y^{2}} + \n",
    "    \\frac{ u_{i    ,j    ,z + 1} - 2 u_{i  ,j  ,z  } + u_{i    ,j    ,z - 1}}{\\Delta Z^{2}}\n",
    "\\right] \n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "\n",
    "The indexing scheme that I am using a local level is:\n",
    "\n",
    "```C++ \n",
    "for (int k = 1; k < zSize - 1; ++k) //z-direction\n",
    "\t{\n",
    "\t  for (int j = 1; j < ySize - 1; ++j) //y-direction\n",
    "\t  {\n",
    "\t    for (int i = 1; i < xSize - 1; ++i) //x-direction\n",
    "\t    {\n",
    "\t    \tu[(xSize * ySize) * (k    ) + xSize * (j    ) + (i    )];\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "```\n",
    "MPI saves memory contiguously first in the x-direction (column-wise), the y-direction (row-wise), and then the z-direction (depth-wise). This schema helps convert the 3D data into a single array.  \n",
    "\n",
    "This is possible since total index of y increases by a factor of xSize because each row has *x* elements, and the total index of z increases by a factor of xSize * ySize because there are *x \\* y* elements in each page of the. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "The derived data types that I made for the XY, YZ, and XZ plane are:\n",
    "```C++\n",
    "  MPI_Datatype XZ, YZ, XY; \n",
    "\n",
    "  //This can be contiguous since the MPI can read the data across the \n",
    "  //X row then up the Y rows.\n",
    "  MPI_Type_contiguous(xSize * ySize, MPI_DOUBLE, &XY);\n",
    "\n",
    "\n",
    "  //Note that for these datatypes, the count * blocklength should be equal to the\n",
    "  //number of elements in the desired plane.\n",
    "\n",
    "  //Creation of the YZ datatype. This will be a vector type that simply \n",
    "  //reads up the column by skipping up through the xSize columns.\n",
    "  MPI_Type_vector(zSize * ySize,     1,         xSize, MPI_DOUBLE, &YZ);\n",
    "\n",
    "  //Creation of the XZ datatype. This will be a vector type that skips \n",
    "  //up the entirity of the XZ plane to get back to the base row. \n",
    "  MPI_Type_vector(zSize, xSize, xSize * ySize, MPI_DOUBLE, &XZ);\n",
    "\n",
    "  //Don't forget to commit your changes to the actual variable.\n",
    "  MPI_Type_commit(&XY);\n",
    "  MPI_Type_commit(&YZ);\n",
    "  MPI_Type_commit(&XZ);\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
